{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s8Sg2V-cTB8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question and Answer"
      ],
      "metadata": {
        "id": "WMTQMWL3TDAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "  >A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that best separates data points of different classes in a high-dimensional space, maximizing the margin between the closest points (support vectors) of each class to ensure robust and accurate predictions.\n",
        "\n",
        "# 2. Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "  >Hard Margin SVM is a type of Support Vector Machine that assumes the data is linearly separable and aims to find a hyperplane that perfectly separates the classes without any misclassification, enforcing strict boundaries.\n",
        "\n",
        "  >Soft Margin SVM allows for some misclassification or overlap between classes by introducing a penalty for errors, making it suitable for non-linearly separable data and improving generalization by balancing margin maximization and classification accuracy.\n",
        "\n",
        "# 3.: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "  >The Kernel Trick in Support Vector Machines (SVM) is a mathematical technique that enables the algorithm to operate in a high-dimensional feature space without explicitly computing the coordinates of the data in that space, allowing SVM to solve non-linear classification problems by applying a kernel function.\n",
        "\n",
        "  >An example is the Radial Basis Function (RBF) kernel, which is commonly used when the relationship between class labels and features is non-linear. It maps input features into an infinite-dimensional space, making it effective for complex decision boundaries in tasks like image classification or bioinformatics.\n",
        "\n",
        "# 4. What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "  >A Naïve Bayes Classifier is a probabilistic machine learning algorithm based on Bayes’ Theorem, used for classification tasks. It assumes that the features in a dataset are conditionally independent given the class label, which simplifies computation and enables efficient prediction.\n",
        "\n",
        "  >It is called “naïve” because of this strong and often unrealistic assumption of feature independence, which rarely holds true in real-world data but still yields surprisingly effective results.\n",
        "\n",
        "# 5.Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.When would you use each one?\n",
        "\n",
        "  >Gaussian Naïve Bayes is a variant of the Naïve Bayes classifier that assumes continuous features follow a normal (Gaussian) distribution. It is best used when the input features are real-valued and continuous, such as in medical data or sensor readings.\n",
        "\n",
        "  >Multinomial Naïve Bayes is designed for discrete count data and assumes features represent the frequency of events. It is commonly used in text classification tasks like spam detection or document categorization, where features are word counts or term frequencies.\n",
        "\n",
        "  >Bernoulli Naïve Bayes models binary/boolean features, assuming each feature is either present or absent. It is suitable for tasks like sentiment analysis or email classification where features indicate the presence or absence of specific terms.\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "iWQ70vsxTHuw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G2-0i56S5Yi",
        "outputId": "e43ced23-161c-4b39-bdd6-39c4d09b64c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ],
      "source": [
        "# 6. Write a Python program to:\n",
        "# Load the Iris dataset\n",
        "# Train an SVM Classifier with a linear kernel\n",
        "# Print the model's accuracy and support vectors.\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM classifier with a linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = svm_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy and support vectors\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(\"Support Vectors:\")\n",
        "print(svm_model.support_vectors_)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 7.Write a Python program to:\n",
        "# Load the Breast Cancer dataset\n",
        "# Train a Gaussian Naïve Bayes model\n",
        "# Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuP7fpybVmUK",
        "outputId": "a1317e6d-1d86-4867-8d4c-73cf24a1381f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        63\n",
            "      benign       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Write a Python program to:\n",
        "# Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.\n",
        "# Print the best hyperparameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define parameter grid for C and gamma\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']  # Using RBF kernel for non-linear classification\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and evaluate accuracy\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuAhUL7dV9SR",
        "outputId": "b7cf7df8-b0ef-4d62-dbdc-7019e421e151"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Model Accuracy: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.Write a Python program to:\n",
        "# Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups).\n",
        "# Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load a subset of the 20 Newsgroups dataset (binary classification for ROC-AUC)\n",
        "categories = ['sci.space', 'rec.sport.baseball']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "y = newsgroups.target\n",
        "\n",
        "# Binarize labels for ROC-AUC\n",
        "y_binary = label_binarize(y, classes=[0, 1])\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Naïve Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Predict probabilities\n",
        "y_proba = nb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gevAUCn7WwyX",
        "outputId": "9943108f-dc72-42e8-94e2-36b72cde98d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Imagine you’re working as a data scientist for a company that handles email communications.\n",
        "#Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:\n",
        "# Text with diverse vocabulary\n",
        "# Potential class imbalance (far more legitimate emails than spam)\n",
        "# Some incomplete or missing data\n",
        "#Explain the approach you would take to:\n",
        "# Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "# Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "# Address class imbalance\n",
        "# Evaluate the performance of your solution with suitable metrics\n",
        "# And explain the business impact of your solution.\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "# Load synthetic email data (spam vs. not spam)\n",
        "categories = ['rec.sport.hockey', 'talk.politics.misc']  # Simulating spam vs. legit\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Handle missing data\n",
        "emails = [text if text else \"no content\" for text in data.data]\n",
        "\n",
        "# Vectorize text using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(emails)\n",
        "y = data.target\n",
        "\n",
        "# Binarize labels for ROC-AUC\n",
        "y_binary = label_binarize(y, classes=[0, 1])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Naïve Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECFV-Vs7XBQh",
        "outputId": "3bfe06e6-ebcc-4284-c914-8ba0a86e3c5e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.99      0.95       309\n",
            "           1       0.98      0.88      0.93       224\n",
            "\n",
            "    accuracy                           0.94       533\n",
            "   macro avg       0.95      0.93      0.94       533\n",
            "weighted avg       0.95      0.94      0.94       533\n",
            "\n",
            "ROC-AUC Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JB_xQLL1YPIx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}